{
	"name": "CDS_Raw_daily",
	"properties": {
		"folder": {
			"name": "CDS"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "apsmaddev01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8abc9863-3cd9-4d6b-ad58-c02307fd30a1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5a820b59-1e08-41d5-9963-3447819271db/resourceGroups/rg-tccc-marketing-analytics-cx-dev/providers/Microsoft.Synapse/workspaces/syn-tccc-mad-use2-dev-01/bigDataPools/apsmaddev01",
				"name": "apsmaddev01",
				"type": "Spark",
				"endpoint": "https://syn-tccc-mad-use2-dev-01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apsmaddev01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#STORAGE_ACCOUNT_NAME      =  \"sause2tcccmaddev0001\"                  # ADLS Storage account\r\n",
					"#DATA_CONTAINER_NAME       =  \"cxanalytics-raw\"                       # container in which Raw data has to be placed\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"password_key = \"password\"\r\n",
					"storage_account = \"storage_account_name\"\r\n",
					"raw_container = \"data_container_name\"\r\n",
					"curated_container = \"data_container_name_cur\"\r\n",
					"jdbcHostname = \"host_name\"\r\n",
					"jdbcDatabase = \"database_name\"\r\n",
					"jdbcPort = \"port\"\r\n",
					"username = \"user\""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from functools import reduce\r\n",
					"import pycountry\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.functions import col, lit, regexp_replace, regexp_extract, initcap\r\n",
					"spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\r\n",
					"def dailyload():\r\n",
					"    yesterday = datetime.now() - timedelta(2)\r\n",
					"    date = datetime.strftime(yesterday, \"%m%d%Y\")\r\n",
					"    newdate = datetime.strftime(yesterday, \"%m-%d-%Y\")\r\n",
					"    file_relative_path        =  \"/CDS/{}/\".format(date)\r\n",
					"    desired_date_folder_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (raw_container, storage_account, file_relative_path)\r\n",
					"\r\n",
					"    print(desired_date_folder_path)\r\n",
					"    try:\r\n",
					"        desired_date_files = mssparkutils.fs.ls(desired_date_folder_path)\r\n",
					"        print(desired_date_files)\r\n",
					"    except Exception as e:\r\n",
					"        print(\"{} folder does not yet exist\".format(desired_date_folder_path))\r\n",
					"        raise e\r\n",
					"    else:\r\n",
					"        fileNames = []\r\n",
					"        lst = ['APAC', 'CDS_NA', 'EMEA', 'LATAM','LATAM_B2C']\r\n",
					"        for i in desired_date_files:\r\n",
					"            fileNames.append(i.name)\r\n",
					"        print(fileNames)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"    if not fileNames:\r\n",
					"        mssparkutils.notebook.exit('stop')\r\n",
					"    else:\r\n",
					"        output = []\r\n",
					"        for file in fileNames:\r\n",
					"            df = spark.read.format(\"csv\").option(\"header\", \"true\").load(desired_date_folder_path+\"/\"+file)\r\n",
					"            if \"Logins\" not in df.columns:\r\n",
					"                df = df.withColumn(\"Logins\", lit(None))\r\n",
					"            if \"newUsers\" not in df.columns:\r\n",
					"                df = df.withColumn(\"newUsers\", lit(None))\r\n",
					"                #print(df)\r\n",
					"            df = df.withColumn(\"Registrations\",df.Registrations.cast('int')).withColumn(\"newUsers\",df.newUsers.cast('int')).withColumn(\"Marketing\",df.Marketing.cast('int')).withColumn(\"UniqueLoggedIn\",df.UniqueLoggedIn.cast('int')).withColumn(\"Personalization\",df.Personalization.cast('int')).withColumn(\"Age\",df.Age.cast('int')).select(\"PropertyName\", \"Country\", \"Age\", \"Registrations\",\"newUsers\",\"Logins\", \"UniqueLoggedIn\", \"Marketing\", \"Personalization\")\r\n",
					"            df1 = df.select(\"PropertyName\", \"Country\", \"Age\", \"Registrations\",\"newUsers\",\"Logins\", \"UniqueLoggedIn\", \"Marketing\", \"Personalization\")\r\n",
					"            df = df1\r\n",
					"            if df.count() == 0:\r\n",
					"                print(\"{} file is empty\".format(file))\r\n",
					"\r\n",
					"            else:\r\n",
					"                if \"UniqueLoggedIn\" not in df.columns:\r\n",
					"                    df = df.withColumn(\"UniqueLoggedIn\", lit(None))\r\n",
					"                output.append(df) \r\n",
					"            \r\n",
					"\r\n",
					"    finaloutput = reduce(DataFrame.unionAll, output)\r\n",
					"    finaldata = finaloutput.withColumn(\"Date\", lit(newdate))\r\n",
					"    \r\n",
					"\r\n",
					"    output = finaldata.dropDuplicates()\r\n",
					"    #writeSingleCsvFile(output, desired_date_folder_path_cur, \"cds_agg_output\")\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"    #jdbcHostname = host_name \r\n",
					"    #jdbcDatabase = database_name\r\n",
					"    #jdbcPort = port\r\n",
					"    #username = user\r\n",
					"    password = password_key\r\n",
					"    jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
					"\r\n",
					"    connectionProperties = {\r\n",
					"        \"user\" : username,\r\n",
					"        \"password\" : password,\r\n",
					"        \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
					"        }\r\n",
					"    \r\n",
					"    dbtable = \"RAW.CDS\"\r\n",
					"    finaldata.write \\\r\n",
					"        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
					"        .mode(\"append\") \\\r\n",
					"        .option(\"url\", jdbcUrl) \\\r\n",
					"        .option(\"dbtable\", dbtable) \\\r\n",
					"        .option(\"user\", username) \\\r\n",
					"        .option(\"password\", password) \\\r\n",
					"        .option(\"header\", \"True\") \\\r\n",
					"        .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\") \\\r\n",
					"        .save()\r\n",
					"    print(\"Done\")"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dailyload()"
				]
			}
		]
	}
}